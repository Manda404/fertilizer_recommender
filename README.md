
---
# ğŸŒ± Fertilizer Recommender  
**Predicting Optimal Fertilizers â€“ Kaggle Competition**

---

## ğŸ¯ Objectif du projet

Ce projet vise Ã  **prÃ©dire le meilleur engrais** Ã  recommander en fonction :
- des conditions mÃ©tÃ©orologiques,
- du sol,
- du type de culture,

dans le cadre de la compÃ©tition Kaggle  
**â€œPredicting Optimal Fertilizersâ€** (mÃ©trique : **MAP@3**).

> âš ï¸ Ce dÃ©pÃ´t ne se limite pas Ã  produire un score Kaggle.  
> Il dÃ©montre une **approche professionnelle, MLOps-ready**, basÃ©e sur la **Clean Architecture**, reproductible et maintenable.

---

## ğŸ§  Principes clÃ©s

### Clean Architecture appliquÃ©e au Machine Learning

Les dÃ©pendances suivent **strictement** le sens :

```

presentation â†’ application â†’ domain
infrastructure â†’ domain (implÃ©mente les ports)

````

- `domain` : rÃ¨gles mÃ©tier pures (aucune lib externe)
- `application` : orchestration des cas dâ€™usage
- `infrastructure` : pandas, sklearn, CatBoost, MLflow, I/O
- `presentation` : CLI / notebooks / API

ğŸ‘‰ **Le domaine ne dÃ©pend de rien.**

---

## ğŸ“‚ Structure du projet

```text
fertilizer_recommender/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # DonnÃ©es Kaggle originales (IMMUTABLES)
â”‚   â”œâ”€â”€ processed/      # Datasets internes (train/test labellisÃ©s)
â”‚   â””â”€â”€ submissions/    # Fichiers submission Kaggle
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ models/         # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ reports/        # Rapports dâ€™Ã©valuation
â”‚   â””â”€â”€ plots/          # Visualisations
â”‚
â”œâ”€â”€ configs/             # Configurations YAML (reproductibilitÃ©)
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ fertilizer_recommender/
â”‚       â”œâ”€â”€ domain/         # CÅ“ur mÃ©tier pur
â”‚       â”œâ”€â”€ application/    # Use cases (orchestration)
â”‚       â”œâ”€â”€ infrastructure/ # ImplÃ©mentations concrÃ¨tes
â”‚       â”œâ”€â”€ presentation/   # Interfaces utilisateur
â”‚       â””â”€â”€ composition_root.py
â”‚
â”œâ”€â”€ tests/               # Tests unitaires et E2E
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
````

---

## ğŸ”„ Cycle de vie des donnÃ©es (trÃ¨s important)

Le **test.csv Kaggle nâ€™est jamais utilisÃ© pendant lâ€™entraÃ®nement**.

```text
data/raw/train.csv   (Kaggle original)
        â”‚
        â–¼
PrepareDatasetUseCase
        â”‚
        â”œâ”€â”€ data/processed/train.csv   â† entraÃ®nement (avec cible)
        â””â”€â”€ data/processed/test.csv    â† validation interne (avec cible)
```

### Pourquoi cette approche ?

* Ã©viter toute fuite de donnÃ©es
* permettre le calcul rÃ©el de la mÃ©trique MAP@3
* figer les datasets comme **artefacts ML**
* rendre le pipeline auditable et reproductible

---

## âš™ï¸ Use cases principaux

### 1ï¸âƒ£ PrepareDatasetUseCase

* Charge `data/raw/train.csv`
* Effectue un split **stratifiÃ©**
* Sauvegarde deux CSV labellisÃ©s :

  * `processed/train.csv`
  * `processed/test.csv`

ğŸ‘‰ Aucun `X/y` manipulÃ© ici :
le split se fait **au niveau DataFrame complet**.

---

### 2ï¸âƒ£ TrainModelUseCase

* Charge `processed/train.csv`
* Applique le preprocessing
* EntraÃ®ne un modÃ¨le multiclasses
* Calcule la mÃ©trique mÃ©tier **MAP@3**
* Sauvegarde le modÃ¨le entraÃ®nÃ©

ğŸ‘‰ Le modÃ¨le et le preprocessing sont **remplaÃ§ables sans modifier le use case**.

---

### 3ï¸âƒ£ EvaluateModelUseCase *(Ã  venir)*

* Ã‰value le modÃ¨le sur `processed/test.csv`
* Produit MAP@3 + rapports
* GÃ©nÃ¨re des artefacts dâ€™analyse

---

### 4ï¸âƒ£ BuildSubmissionUseCase *(Ã  venir)*

* Charge le modÃ¨le entraÃ®nÃ©
* PrÃ©dit le **Top-3** dâ€™engrais
* GÃ©nÃ¨re `submission.csv` au format Kaggle

---

## ğŸ“ MÃ©trique mÃ©tier â€“ MAP@3

La mÃ©trique officielle Kaggle est implÃ©mentÃ©e :

* en **Python pur**
* dans le **domain**
* sans dÃ©pendance externe

ğŸ‘‰ Cela garantit :

* la testabilitÃ©
* la transparence
* lâ€™alignement exact avec Kaggle

---

## ğŸ§ª Tests

```text
tests/
â”œâ”€â”€ domain/        # rÃ¨gles mÃ©tier
â”œâ”€â”€ application/   # use cases
â”œâ”€â”€ infrastructure/
â””â”€â”€ e2e/           # pipeline complet
```

Les tests couvrent :

* la validitÃ© mÃ©tier
* la stabilitÃ© des use cases
* lâ€™exÃ©cution complÃ¨te du pipeline ML

---

## ğŸ” ReproductibilitÃ© & MLOps

* Configurations YAML (seed, paths, modÃ¨les)
* Datasets figÃ©s (`processed/`)
* Artefacts versionnÃ©s (`artifacts/`)
* Architecture extensible vers :

  * MLflow
  * Model Registry
  * CI/CD

---

## ğŸš€ Lancer le projet (local)

```bash
# PrÃ©parer les datasets
python -m fertilizer_recommender.presentation.cli.prepare_dataset

# EntraÃ®ner le modÃ¨le
python -m fertilizer_recommender.presentation.cli.train

# GÃ©nÃ©rer une submission Kaggle
python -m fertilizer_recommender.presentation.cli.submit
```

---

## ğŸ§© Pourquoi ce projet est diffÃ©rent

âœ”ï¸ Clean Architecture rÃ©elle (pas cosmÃ©tique)
âœ”ï¸ SÃ©paration stricte des responsabilitÃ©s
âœ”ï¸ Pipeline ML explicable et auditable
âœ”ï¸ Approche proche des standards entreprise
âœ”ï¸ ConÃ§u pour Ãªtre maintenu, testÃ© et Ã©tendu

---

## ğŸ‘¤ Auteur

Projet dÃ©veloppÃ© dans une optique **Data Science / MLOps professionnelle**,
avec une attention particuliÃ¨re portÃ©e Ã  :

* la qualitÃ© du design
* la reproductibilitÃ©
* la lisibilitÃ© long terme

---

> *â€œUn bon modÃ¨le fait un score.
> Un bon systÃ¨me ML survit dans le temps.â€*

```

---

## Ce README est :

- cohÃ©rent avec **tout ce que tu as construit**
- dÃ©fendable en entretien
- lisible par un non-Kaggleur
- parfaitement alignÃ© Clean Architecture

ğŸ‘‰ **Ã‰tape suivante** possible :
- README technique par couche (`domain/README.md`)
- Diagramme dâ€™architecture
- Ajout MLflow dans le README
- Section â€œDesign Decisionsâ€ (trÃ¨s senior)

Dis-moi ce que tu veux enrichir ensuite ğŸ‘Œ
```
